{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting stock price intraday data from Alpha Vantage into 12 csv files\n",
    "import requests\n",
    "import time\n",
    "\n",
    "base_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=TSLA&interval=1min&slice=year1month{}&apikey=XXXXXXX'\n",
    "\n",
    "for month in range(1, 13):\n",
    "    url = base_url.format(month)\n",
    "    response = requests.get(url)\n",
    "    with open(f'tsla_1min_{month}.csv', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    time.sleep(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many csv to 1\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "input_dir = 'Project/alphav TSLA 1min/april 2022 -2023'\n",
    "\n",
    "input_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "\n",
    "df = pd.concat([pd.read_csv(f) for f in input_files])\n",
    "\n",
    "output_file = '2022-2023_1min.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting ulrs of media files for AWS rekognition\n",
    "apify_15_23 = pd.read_csv('apify_15_23_all.csv')\n",
    "\n",
    "with open('ids_urls.csv', 'w') as f_ids:\n",
    "    f_ids.write('id,url\\n')\n",
    "    for _, row in apify_15_23.iterrows():\n",
    "        urls = [row[f'media/{i}/media_url'] for i in range(4)]\n",
    "        urls = [url for url in urls if  isinstance(url, str)]\n",
    "        id = row['id']\n",
    "        for url in urls:\n",
    "            f_ids.write(f'{id},{url}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS rekognition request -> to csv\n",
    "import boto3\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n",
    "\n",
    "df_links = pd.read_csv('ids_urls.csv')\n",
    "\n",
    "columns = ['ID', 'Link', 'Detect Labels', 'Detect Faces', 'Recognize Celebrities', 'Detect Text']\n",
    "dtypes = {'ID': int, 'Link': str, 'Detect Labels': str, 'Detect Faces': str, 'Recognize Celebrities': str, 'Detect Text': str}\n",
    "df = pd.DataFrame(columns=columns).astype(dtypes)\n",
    "\n",
    "session = boto3.session.Session(aws_access_key_id='XXXXXXXXXXXXXXXXX')\n",
    "\n",
    "rekognition_client = session.client(service_name='rekognition', region_name='eu-west-1')\n",
    "\n",
    "for i in range(len(df_links)):\n",
    "    row = df_links.iloc[i]\n",
    "    link = row['url']\n",
    "    print(link)\n",
    "\n",
    "    response = requests.get(link, verify=False)\n",
    "    imgbytes = response.content\n",
    "\n",
    "    print('DETECT_LABELS')\n",
    "    response = rekognition_client.detect_labels(Image={'Bytes': imgbytes}, MaxLabels=10, MinConfidence=90)\n",
    "    labels = response['Labels']\n",
    "    \n",
    "    print('DETECT_FACES')\n",
    "    response = rekognition_client.detect_faces(Image={'Bytes': imgbytes})\n",
    "    faceDetails = response['FaceDetails']\n",
    "    \n",
    "    print('RECOGNIZE_CELEBRITIES')\n",
    "    response = rekognition_client.recognize_celebrities(Image={'Bytes': imgbytes})\n",
    "    celebrityFaces = response['CelebrityFaces']\n",
    "    \n",
    "    print('DETECT_TEXT')\n",
    "    response = rekognition_client.detect_text(Image={'Bytes': imgbytes})\n",
    "    textDetections = response['TextDetections']\n",
    "    \n",
    "    row_data = {'ID': row['id'], 'Link': link, 'Detect Labels': labels, 'Detect Faces': faceDetails,\n",
    "                'Recognize Celebrities': celebrityFaces, 'Detect Text': textDetections}\n",
    "    df = df.append(row_data, ignore_index=True)\n",
    "    \n",
    "    print('')\n",
    "\n",
    "df.to_csv('rekognition_elon.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS rekognition results processing\n",
    "df = pd.read_csv('rekognition_elon.csv')\n",
    "\n",
    "# same celeb twice on picture (set not gonna work, 'cause we want to extract their face emotions too)\n",
    "def remove_duplicates(records):\n",
    "    unique_names = set()\n",
    "    unique_records = []\n",
    "    for record in records:\n",
    "        name = record['Name']\n",
    "        if name not in unique_names:\n",
    "            unique_names.add(name)\n",
    "            unique_records.append(record)\n",
    "    return unique_records    \n",
    "\n",
    "df['Labels'] = df['Detect Labels'].apply(lambda x: [d['Name'] for d in eval(x)])        \n",
    "df['Unique Celeb'] = df['Recognize Celebrities'].apply(eval).apply(remove_duplicates)\n",
    "df['Celeb_uniq'] = df['Unique Celeb'].apply(lambda x: [d['Name'] for d in x])\n",
    "df['Emotion'] = df['Unique Celeb'].apply(lambda x: [d['Face']['Emotions'][0]['Type'] for d in x])\n",
    "df['Smile'] = df['Unique Celeb'].apply(lambda x: [d['Face']['Smile']['Value'] for d in x])\n",
    "df['Gender'] = df['Unique Celeb'].apply(lambda x: [d['KnownGender']['Type'] for d in x])\n",
    "\n",
    "\n",
    "normalized_df = pd.DataFrame()\n",
    "for _, row in df.iterrows():\n",
    "    celeb_uniq = row['Celeb_uniq']\n",
    "    emotions = row['Emotion']\n",
    "    smiles = row['Smile']\n",
    "    genders = row['Gender']\n",
    "    for celeb, emotion, smile, gender in zip(celeb_uniq, emotions, smiles, genders):\n",
    "        normalized_df = normalized_df.append({\n",
    "            'Celeb_uniq': celeb,\n",
    "            'Emotion': emotion,\n",
    "            'Smile': smile,\n",
    "            'Gender': gender,\n",
    "            'Link': row['Link'],\n",
    "            'ID': row['ID']\n",
    "        }, ignore_index=True)\n",
    "normalized_df.to_csv('celebs.csv',index=False)\n",
    "\n",
    "main_label = pd.DataFrame({\n",
    "    'Label': df['Labels'].apply(lambda x: x[0] if x else None),\n",
    "    'Link':df['Link'],\n",
    "    'ID': df['ID']\n",
    "    })\n",
    "main_label.to_csv('main label.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions / sentiment etc. in tweets\n",
    "import tweetnlp\n",
    "\n",
    "model = tweetnlp.load_model('topic_classification', multi_label=False)\n",
    "df['topic'] = df['full_text'].apply(lambda x: model.topic(x)['label'])\n",
    "\n",
    "irony = tweetnlp.load_model('irony')\n",
    "df['irony'] = df['full_text'].apply(lambda x: irony.irony(x)['label'])\n",
    "\n",
    "emotion = tweetnlp.load_model('emotion')\n",
    "df['emotion'] = df['full_text'].apply(lambda x: emotion.emotion(x)['label'])\n",
    "\n",
    "sent = tweetnlp.load_model('sentiment')\n",
    "df['sentiment'] = df['full_text'].apply(lambda x: sent.sentiment(x)['label'])\n",
    "\n",
    "off = tweetnlp.load_model('offensive')\n",
    "df['offensive'] = df['full_text'].apply(lambda x: off.offensive(x)['label'])\n",
    "\n",
    "\n",
    "# trying another model for emotions to compare\n",
    "# Jochen Hartmann, \"Emotion English DistilRoBERTa-base\". https://huggingface.co/j-hartmann/emotion-english-distilroberta-base/, 2022.\n",
    "# (7 emotions: anger ðŸ¤¬, disgust ðŸ¤¢, fear ðŸ˜¨, joy ðŸ˜€, neutral ðŸ˜, sadness ðŸ˜­, surprise ðŸ˜²)\n",
    "from transformers import pipeline\n",
    "emotion = pipeline(model='j-hartmann/emotion-english-distilroberta-base')\n",
    "df['emotion_hartmann'] = df['full_text'].apply(emotion)\n",
    "df['emotion_hartmann_'] = df['emotion_hartmann'].apply(lambda x: x[0]['label'])\n",
    "df['score_hartmann'] =  df['emotion_hartmann'].apply(lambda x: x[0]['score'])\n",
    "df.drop('emotion_hartmann', axis=1, inplace=True)\n",
    "df.rename(columns={'emotion_hartmann_': 'emotion_hartmann'}, inplace=True)\n",
    "\n",
    "df.to_csv('tweets_sentiment.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets on Elon Musk Twitter Acquisition (July 2022)\n",
    "# https://www.kaggle.com/datasets/aneeshtickoo/tweets-on-elon-musk-twitter-acquisition\n",
    "\n",
    "# has a lot of spam, so removing tweets with only links, + delete duplicates as those are also spam/news headers\n",
    "# 72k -> 44k tweets\n",
    "import re\n",
    "\n",
    "deal = pd.read_csv('Project/twitter_deal.csv')\n",
    "deal = deal.dropna(axis=1, how='all')\n",
    "\n",
    "deal['tweet'] = deal['tweet'].str.replace(r'\\B#\\w+\\b', '')  # remove hashtags\n",
    "deal['tweet'] = deal['tweet'].str.replace(r'http\\S+|www\\S+', '', flags=re.MULTILINE)  # remove links\n",
    "deal['tweet'] = deal['tweet'].str.strip()\n",
    "\n",
    "deal_clean = deal[deal['tweet'] != ''].copy() # drop empty tweets\n",
    "deal_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "spam_pattern = r'extra \\d+% off|save \\$ with'\n",
    "deal_clean = deal_clean[~deal_clean['tweet'].str.contains(spam_pattern)].copy()\n",
    "\n",
    "deal_clean.drop_duplicates(subset=['tweet'], inplace=True)\n",
    "deal_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "deal_clean.to_csv('deal_clean.csv',index=False)\n",
    "\n",
    "# sentiments\n",
    "deal_clean['sentiment'] = deal_clean['tweet'].apply(lambda x: sent.sentiment(x)['label'])\n",
    "deal_clean['emotion'] = deal_clean['tweet'].apply(lambda x: emotion.emotion(x)['label'])\n",
    "deal_clean['irony'] = deal_clean['tweet'].apply(lambda x: irony.irony(x)['label'])\n",
    "deal_clean['offensive'] = deal_clean['tweet'].apply(lambda x: off.offensive(x)['label'])\n",
    "\n",
    "# hartmann model had an error with 1 row, so we catch it\n",
    "def process_tweet(tweet):\n",
    "    try:\n",
    "        return emotion(tweet)\n",
    "    except RuntimeError:\n",
    "        return None\n",
    "    \n",
    "deal_clean['emotion_hartmann'] = deal_clean['tweet'].apply(process_tweet)\n",
    "deal_clean['emotion_hartmann_'] = deal_clean['emotion_hartmann'].apply(lambda x: x[0]['label'] if x else None)\n",
    "deal_clean['score_hartmann'] = deal_clean['emotion_hartmann'].apply(lambda x: x[0]['score'] if x else None)\n",
    "deal_clean.drop('emotion_hartmann', axis=1, inplace=True)\n",
    "deal_clean.rename(columns={'emotion_hartmann_': 'emotion_hartmann'}, inplace=True)\n",
    "\n",
    "deal_clean.to_csv('deal_sent.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
